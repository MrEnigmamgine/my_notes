[https://ds.codeup.com/clustering/project/](https://ds.codeup.com/clustering/project/)  
  
Turn in a link to your FINAL NOTEBOOK on GitHub. Not the repo, the FINAL NOTEBOOK. Also attach link to your video recording.  
Remember your helper files should be in the top level of your repo, and all files to be graded should be named appropriately.


# Presentation (live)
## Introduction communicated clearly when presenting
Speaker kicks off the presentation by introducing themselves and their project through a one-liner of what it's about.  

2 points

- Great Intro
    > 2 pts    
    > You kicked off your presentation by introducing yourself and a one-liner of what your project is about.

- Intro missing key items
    > 0 pts    
    > You either didn't introduce yourself, your project, or both.


## Presentation appropriate for audience & setting

Always be aware of the audience and setting for your presentation. What is the appropriate level of technicality? What is the appropriate depth given audience, setting and medium in which its delivered. The way you communicate should be appropriate for the audience: volume, speed of talk, flow, professionalism. (Codeup Data Science Instructor Team, virtually delivered via jupyter notebook).

/2

- Considered audience/setting

    > 2 pts    
    > You clearly considered the audience and setting in which the presenation was delivered. The language was professional, volume and speed of talk was appropriate, as was level of depth, and it was easy to follow you throughout the presentation.


- Improvement needed

    > 0 pts    
    > Improvement needed in your communication consideration toward the audience and setting for the presentation of this project. You may have gone into too much depth in some areas, communication may need to be more professional, volume or speed of your talk may need some adjustment, or the presentation (whether via slides or notebook) was hard to see given the setting it was delivered in.



## Report content is relevant

Notebook talked through step-by-step, in an understandable and meaningful way. Extraneous content in the notebook is not present.  
/2


- Notebook matched presentation

    > 2 pts  
    > Notebook was clean and relevant such that what was talked about what easy to see in the notebook, and what was in the notebook was relevant information that was included in the presentation.


- Notebook or presentation lacked organization or harmony

    > 0 pts  
    > Notebook was NOT talked through in a step-by-step, understandable and meaningful way, highlighting the key steps and discoveries. This can happen with the final notebook contains too much information that may have been relevant in your discovery, but is not relevant for the final report, such as plots of all the distributions, e.g. What is in the notebook should be talked about and what is talked about, should be the primary content of the notebook.



## Conclusion communicated clearly when presenting

Presentation is concluded with a summary of what was found, recommendations, and next steps. The presentation does not just drop off after modeling, but the entire project is nicely tied up and summarized.  
/2


- Conclusion with summary

    > 2 pts    
    > Your presentation concluded with a summary of what was found, recommendations, and next steps.


- No Final Summary

    > 0 pts    
    > Your presentation lacked a summary of what was found, recommendations, and next steps.

## Presentation time limit respected

Time limit is adhered to. The time is managed well, in that there is appropriate time spent on each section. The time should not be met by talking quickly but by reducing the amount or depth of information conveyed, and by finding easier and more simplified methods to convey the more complex information. The speech should be natural, and take the time needed for the audience to consume the information. So the time is well spent when you have practiced and you have taken the extra time it takes to reduce the content in your notebook and presentation. Time should not be spent scrolling through 10's of visualizations or hundreds of lines of code.  
/2


- timely and planned

    > 2 pts  

- time not well planned

    > 0 pts  


# Plan (readme)
## Readme includes project planning

Your readme should include a project plan which helps guide both the user and yourself through the different stages of the pipeline and steps you took to get to your conclusion.
/2


- Includes the project 'How'

    > 2 pts    
    > Your readme includes a project plan which helps guide both the user and yourself through the different stages of the pipeline and steps you took to get to your conclusion


- Missing the 'How'

    > 0 pts    
    > Your readme is missing a complete project plan which helps guide both the user and yourself through the different stages of the pipeline and steps you took to get to your conclusion.



## Readme includes data dictionary

Your readme should include a data dictionary, which is important to provide in order to define and disambiguate each of the variables you are analyzing.
/2


- Includes complete data dictionary

    > 2 pts    
    > Your readme includes a complete and accurate data dictionary


- Missing a complete and accurate data dictionary

    > 0 pts    
    > You are missing a complete and accurate data dictionary.

## Readme includes steps to reproduce

Your readme should include useful and adequate instructions for reproducing your analysis and final report.
/2


- Complete instructions to reproduce

    > 2 pts    
    > Reproducible with ease!


- Instructions lacking

    > 0 pts    
    > You were either missing the steps to reproduce or the steps you did provide were not adequate for reproduction of your project with data, modules and notebook.


## Readme includes project goal(s)

Your readme should include a clear project goal that reflect on what you are trying to achieve for the business/organization (in the scenario layed out). Your goal is never just to create an algorithm or model or to purely make discoveries in exploration...it includes the why. 'My goal is to..., so that...' Your goal should be specific enough to know when you have reached it and concise enough to keep in 1-2 sentences. 'My goal is to identify key drivers of churn, which customers at risk of churn, and make recommendations for changes so that we can reduce the monthly churn rate and increase customer retention.' This helps tell you when you have reached a minimally viable product (including a presentation and predictions, in this example scenario).
/2


- Goals include what and why

    > 2 pts    
    > You stated clear and concise project goals and included the 'so what'.


- Goals missing, incomplete, or inaccurate.

    > 0 pts    
    > You did not state clear and concise project goals, including the 'so what'.


## Readme includes project description

Your readme should include a project description that provides context for your project, including explaining why you are tackling this project, why it is important and how it could be of use to someone else beyond just the interest or new knowledge. It dives in a bit deeper than the goals. Project description and goals should always be in your words and specific to your project, not a copy of the class project spec.
/2


- Project description provides context.

    > 2 pts    
    > Project description provides context and explains the 'whys' to your project. It is NOT a copy of the class project spec.


- Project description missing, incomplete or copy of spec.

    > 0 pts    
    > You are missing a project description, it does not give the context and 'whys' of the project, OR it is a copy of the class project spec.



## Readme includes initial questions/hypotheses

Your readme should include initial questions and focus you are going into the analysis with. This is an important part of project planning and gives context to the reader about where you started, what were your initial ideas and thoughts, and did those play out to be true.
/2


- Initial questions, areas of focus for exploration are discussed

    > 2 pts    
    > In your readme, you discussed the initial questions and focus you are going into the analysis with.


- No initial questions, discussion of areas of focus for analysis.

    > 0 pts    
    > In your readme, you did not discuss the initial questions and focus you went into the analysis with.


# Wrangle (ipynb)

## wrangle(ipynb) - modules used

After creating the wrangle module(s), you import those into your final report, so that you can use those functions you wrote to acquire and prepare your data with ease, with little clutter, and with reduced risk of running into issues when reproducing the report. The functions should be called to prepare your data (as opposed to re-writing the code of the functions in your notebook), and you should include in a markdown cell, a verbal explanation of the steps you took to prepare the data and why you made those decisions.
/2


- Functions used with md documentation.

    > 2 pts    
    > You called the functions from modules that you created to wrangle data in your report, and you included, in your final notebook, a markdown cell, explaining the steps you took to prepare the data and why.


- Missing md documentation, but functions used.

    > 1 pt  
    > You called the functions from modules that you created to wrangle data in your report, but you did not include, in your final notebook, a markdown cell, explaining the steps you took to prepare the data.


- Prep module not used.

    > 0 pts    
    > You did not utilize the functions in your prepare.py or wrangle.py module to wrangle the data. The purpose for building modules is to import functions into your notebook reducing clutter in the final report/notebook.



## wrangle(py ) - modules created with functions for wrangling

Module(s) with user-defined functions for acquiring and preparing the data should be created. Each function contains a helpful docstring explaining what it does, its input(s) and output(s). Credentials (such as in an env.py file) are NOT included in the public repo.
/3


- Modules with functions using docstrings.

    > 3 pts    
    > Module(s) with functions for acquiring and preparing the data are created. Functions include useful docstrings (beyond the docstrings of functions written and shared by instructors)


- Modules with functions, but limited docstrings

    > 1 pt  
    > Module(s) with functions for acquiring and preparing the data are created. Many functions, beyond the docstrings of functions written and shared by instructors, do NOT include useful docstrings.


- Missing functions, or env file in github

    > 0 pts    
    > Module(s) with functions for acquiring and preparing the data are missing OR your env.py file containing credentials was pushed to github, leading to a security risk.



## wrangle(py ) - data split is reproducible

When splitting data into samples, someone should be able to run your code and get the same observations in the same samples, i.e. reproduce your split, because you set the random state to a seed. There should be 3 adequately sized samples - train, validate, and test. As a good starting point, 50%, 30%, and 20%, (or 50%, 26%, 24% for simplicity when doing the splitting) are reasonable split proportions. But that can vary depending on the number of observations you have. Test can go as low as 10% if needed.
/1


- RandomState set

    > 1 pt  
    > Your split is reproducible becuase you set the random state to a seed.


- No RandomState set

    > 0 pts    
    > Your split is not reproducible because you did not set the random\_state to a seed.

##  wrangle(py or ipynb) - scaling performed correctly

If you are performing regression, clustering, or other algorithms that incorporate a distance metric (such as KNN), your features must be on the same scale when modeling. A scaler object must be fit on train and then transformed on all three datasets (train, validate, test).
/3


- Scaling performed correctly

    > 3 pts    
    > You correctly scaled your data, and used scaled data in clustering and regression.


- Scaling fit on entire dataset

    > 1 pt  
    > You scaled your data; however, you fit the scaler on the entire dataset instead of just the train. This can lead to overfitting because you are using data you should not have seen in creating your features.


- Scaling not performed when needed.

    > 0 pts    
    > You used regression, clustering, or other algorithms that incorporated a distance metric, such as KNN, and because you didn't use scaled features, the algorithms did not perform as they should.


## wrangle(py) - data integrity - accurate data acquired /3

Accurate sample was acquired from the mySQL zillow database. Properties\_2017 table was used. Predictions\_2017 was used to filter to properties that had a transaction during 2017 and include the target variable. Properties were then filtered down to the single family homes only. You should have ended with 52,442 properties based on that criteria. Be sure and add a conditional statement to your acquire script to read a local csv of the sample that you saved so you don't have to run the query every time you re-run your notebook!


- 52k properties.

    > 3 pts    
    > Your SQL query returned 52,442 properties...Single Family Properties which had a transaction during 2017.

- Not optimal performance

    > 2 pts    
    > You created unnecessary bandwidth issues that can impact others by either not using SQL to filter the rows (and reading in 1M+ properties, e.g.) or by not adding a conditional statement in your acquisition script and reading a local copy once you have acquired the data already.


- Errors in Query

    > 1 pt  
    > Your SQL query didn't filter accurately due to errors in the query, e.g. join errors.


- No SQL

    > 0 pts    
    > You did not use SQL to acquire the data.


## wrangle(py or ipynb) - missing values addressed

Decisions made and reasons are communicated and documented for handling missing values. If you imputed based on a computed value (such as mean, median, etc), that was done after splitting the data, and the value was derived from the training dataset only and then imputed into all 3 datasets. If you filled missing values with 0 or a constant not derived from existing values, that can be done prior to splitting the data.
/2


- Sound handling of missing values

    > 2 pts    
    > You handled missing values by imputing or removing them in a logical and communicated way. You correctly handled missing values using a sound logic that was communicated via markdown documentation. You correctly used either the training data or the entire dataset to impute (given your method(s) for filling missing values).


- Issues with handling missing values

    > 0 pts    
    > Improper, incomplete, or uncommunicated methods of handling missing values. You either improperly fit the imputer to your entire dataset (not just train), you did not document your decision, or your logic wasn't sound.



## wrangle(ipynb) - data split into 3 samples avoiding data leakage

Data should always be split into three samples (Train, Validate, and Test) prior to exploration of variable relationships. In addition, imputers, scalers, feature elimination, and selection algorithms should all be run after the split, so they are fit on train and transform validate and test.
/3


- Three Samples Before Exploring

    > 3 pts    
    > Your data was split into three samples before exploring the interaction of variables, imputing, scaling, or running feature elimination/selection algorithms.


- Two Samples Before Exploring

    > 2 pts    
    > You split your data into only two samples. It's important to have a validate sample to test for overfitting of any models. Then the test can be used to estimate how the final model will do in the future on unseen data. You did a good job of splitting before exploring the interaction of variables, imputing, scaling, or running feature elimination/selection algorithms.


- Split late, Three Samples

    > 1 pt  
    > You did not split your data at the appropriate time. Data should be split prior to exploration of variable relationships. In addition, imputers, scalers, feature elimination, and selection algorithms should all be run after the split, so that they are fit on train and transform validate and test.


- No Splits OR Late with Two Samples

    > 0 pts    
    > You failed to split your data into train, test, and validate samples. -OR- You split into only 2 samples (missing validate), and you did not split your data at the appropriate time. Data should be split prior to exploration of variable relationships. In addition, imputers, scalers, feature elimination, and selection algorithms should all be run after the split, so that they are fit on train and transform validate and test.



# Explore (ipynb)
## explore(ipynb) - well designed analysis - ask a clear question, \[discover\], provide a clear answer.

Key questions asked and answered of the data are shared in the final report notebook. You should ask questions of the data using natural language that speaks to the business stakeholders in markdown cells, ideally a header prior to the visualization or statistical test, that you then explore. This does not take the place of stating your null hypothesis/alternative hypothesis when doing a statistical test, but those hypotheses are generally for you. By writing questions that you intend to answer with visualizations and statistical tests in natural language, like 'Are office supplies leading to differences in profit in Texas?', you are able to guide both yourself and your reader through the highlights of your analysis. You ask a question, create a visual, run a statistical test (if appropriate), and wrap it nicely with a markdown cell that contains a clear answer in layman's terms. You do all that before moving to the next question.
/6


- Viz's/tests wrapped in a formed Q & A

    > 6 pts  
    > You called out at least four of the questions you asked of the data and provided a clear answer using natural language in markdown cells in your final report.


- Missing 1 Q&A

    > 4 pts  
    > You called out only three of the questions you asked of the data and provided a clear answer using natural language in markdown cells in your final report.


- Missing Q's/Has A's

    > 3 pts  
    > Viz's/tests end with takeways, but they are not introduced with a clear question or goal of the visualization/test


- Missing A's/Has Q's

    > 2 pts  
    > Questions are not answered/conclusions not drawn in your final report in a way you would communicate to all audiences. Answering with 'reject the null hypothesis' is not a satisfactory way to summarize the takeaway.


- Missing 2 Q&A

    > 1 pt
    > You called out only two of the questions you asked of the data and provided a clear answer using natural language in markdown cells in your final report.


- Missing 3-4 Q&A's

    > 0 pts  
    > Majority of viz's/tests you presented in final report are not preceded with a question or goal and are not summarized with an answer/conclusion in a natural language.



## explore(ipynb) - questions answered using visual exploratory analysis

Visualizations are included in your final report. Those included answer a question (remember, 'no' is an answer) or provide necessary context (such as the distribution of the target variable). All statistical tests included in the final report should be supported with a visualization of the interaction of the tested variables. Charts in the final report should have titles and labels that are descriptive and useful for the end user/audience/consumer of the report. All visualizations in the final report are mentioned or discussed if a verbal presentation is given.
/5


- 5-7 Vizs

    > 5 pts  
    > Between five and seven visualizations included. Included visualizations should be tied to specific statements/questions and takeaways/answers (with or without a statistical test) or set the context for the target or other key variable. Including more than seven visualizations in the final report typically indicates some are extraneous.


- 3-4 Vizs

    > 3 pts  
    > Only three or four visualizations included. ncluded visualizations should be tied to specific statements/questions and takeaways/answers (with or without a statistical test) or set the context for the target or other key variable. All statistical tests should be supported by a visualization.


- \>7 Vizs

    > 2 pts  
    > Visualizations shown do not answer questions clearly, are not tied to clearly stated takeaways, do not provide a specified context, or are misleading or incorrect in their representation. In a report, the included visualizations should answer questions, provide a specific context, or support a statistical test, and the included charts should be discussed or mentioned in the presentation (if applicable).


- <3 visualizations created. < 50% of questions are followed by a chart.

    > 0 pts  
    > Visualizations shown do not answer questions clearly or are not tied to clearly stated takeaways. In a report, the visualizations included should answer questions and takeaways should be stated regarding drivers of your outcome. Takeaways or conclusions are written in markdown cells in natural, non-statistical language. Ideally, you want to explain what the takeaway is, right after or before the visual. All statistical tests should be supported by a visualization.



## explore(ipynb) - questions answered using statistical analysis

Statistical tests are included in your final report. The correct tests are run, given the data type and distribution, and the correct conclusions are drawn. - correlation: two continuous variables, normally distributed, testing for LINEAR correlation only (H\_0: Not linearly dependent); - independent t-test: one continuous, somewhat normally distributed variable, one boolean variable, equal variance, independent (H\_0: population mean of each group is equal); - chi-squared test: two discrete variables. (H\_0: the two variables are independent of each other). (other tests may be used)
/5


- 2 accurate tests included

    > 5 pts  
    > You correctly utilize at least two statistical tests (correct variables, conclusions drawn, no assumptions violated, and proper conclusions reached).


- 1 accurate test included

    > 3 pts  
    > You correctly utilize only one statistical test (correct variables, conclusions drawn, no assumptions violated, and proper conclusions reached)


- No statistical tests, or all used/interpreted incorrectly

    > 0 pts  
    > Statistical tests were not performed or were used or interpreted incorrectly.

## explore(ipynb) - clustering used in exploration /6

Clustering is used to further explore the drivers of your target. At least 3 combinations of features are used for identifying clusters. What you do with these clusters is up to you. Remember to see the 'Using Clusters' lesson as a refresher. You may find the clusters you build are not useful at all. Or maybe they are useful in purely analysis, in understanding and being able to explain what is going on, but you have not found them useful in your supervised model. But to determine if they are useful, you must analyze the clusters created! run statistical tests and visualize the clusters, comparing the mean log error across the different clusters. Is there a significant difference between them? Plot them! Put the log error on the y-axis, cluster on th x-axis, and visualize through a violin plot, a catplot, a swarmplot, or others. Other reminders: label your clusters with something meaningful; the numeric ID given to distinguish one cluster from another does not represent a numeric measure, so don't use those values in modeling - dummy variables are the key; you don't have to use every cluster of a clustering model as features, you can select just the one(s) that show significant relationship(s) with the target variable.

- 3 clustering models + explored

    > 6 pts  
    > You developed 3 clustering models and explored the relationship of those clusters to the target variable.


- 1-2 clustering models + explored

    > 5 pts    
    > You developed 1 or 2 clustering models and explored the relationship of those clusters to the target variable.


- 3 clustering models, NOT explored

    > 2 pts    
    > You developed 3 clustering models but did NOT explore the relationship of those clusters to the target variable.


- 1-2 clustering models, NOT explored

    > 1 pt  
    > You developed 1 or 2 clustering models and did NOT explore the relationship of those clusters to the target variable.


- No clustering

    > 0 pts    
    > You did not develop any clustering models.  

## explore(ipynb) - exploration summarized

Following your exploration section, you summarize your analysis (in a markdown cell using natural language): what you found and how you will use it moving forward. This includes key takeaways from all the questions answered in explore, a list of which features will be used in modeling and why, and which features will not move forward and why. You may only call out a few of these features in the presentation, but having that there for reference is important in a report. A group of features may have the same reason why, and those can be mentioned together.
/6


- Analysis Summary + Features

    > 6 pts  
    > Summary of analysis includes key takeaways from the questions answered in explore, and documentation of the features that will be tried out in modeling and why, and which features will not move forward and why. Well done!


- Missing key takeaways, Features included

    > 3 pts  
    > Summary is lacking takeaways from your exploration. You did include a list of which features were moving forward and why, and which ones were not and why.


- Feature summary missing, includes key takeaways.

    > 2 pts  
    > Documentation of the features that will be tried out in modeling and why, and which features will not move forward and why is missing, but you did include key takeaways from your analysis.


- Lacking Summary + Features.

    > 0 pts  
    > Summary of analysis lacks key takeaways and documentation of feature selection decisions.



# Model
## model(ipynb) - in-sample evaluation of a variable-free baseline (train)

A baseline tells you whether a model you build using the features you selected performs any better than predicting by using only the target variable. In classification, a baseline can be created by predicting only the most common outcome class, like predicting that all Titanic passengers will die becuase the majroity died. This results in the highest accuracy without using extra information from features. The baseline is based on the training dataset. For a continuous target variable, the baseline could be predicting that all salaries will be the median salary of our labeled train data. The predictions should be made on the training data (like the predicted value, y\_hat, for all passengers 'survived' == 0) and then evaluated on validate to compare against your models. If any model you build does not perform better than a baseline that uses no features, then your features are not significant drivers of the outcome.
/1


- Baseline predictions evaluated

    > 1 pt
    > You made baseline predictions and evaluated the performance to measure your models against. Establishing a baseline prediction method that uses no features because it tells you if having a model has any value. If any model you build does not perform as well as a baseline that uses no features, then your features are not significant drivers of the outcome.


- Baseline predictions not made/evaluated

    > 0 pts  
    > You did not make baseline predictions and evaluate the performance. Establishing a baseline prediction method that uses no features because it tells you if having a model has any value. If any model you build does not perform as well as a baseline that uses no features, then your features are not significant drivers of the outcome.



## model(ipynb) - in-sample evaluation for model performance (train)

All models should be evaluated on train. The training sample is our largest sample, and we have to both fit the model on train and see how the model performs on it. We should never skip straight to validate. We would be missing out on valuable observations.
/1


- All models evaluated on train

    > 1 pt
    > All models evaluated on train: models are always evaluated on train because it is a sample of data we have to see how the model performs. We should never skip straight to validate. We would be missing out on valuable observations.


- Not all models evaluated on train

    > 0 pts  
    > One or more models were not evaluated on train: models are always evaluated on train because it is a sample of data we have to see how the model performs. We should never skip straight to validate. We would be missing out on valuable observations.



## model(ipynb) - out-of-sample evaluation for overfitting (validate)

The top models should be evaluated with the validation sample dataset. It is important to use the validate sample for checking for any overfitting that may have occurred when fitting the model on train. If you are creating many models, it is also important to only validate a handful of your top models with the validate dataset. Otherwise, your data will have seen validate as much as train, and you could accidentally introduce some implicit bias based on data and results you see while validating so many models.
/1


- Top models evaluated on validate

    > 1 pt
    > Your top models are evaluated with the validate sample to check for overfitting.


- Models not evaluated on validate dataset

    > 0 pts  
    > Your top models are not evaluated with a validation sample dataset. It is important to use the validate sample for checking for any overfitting that may have occurred when fitting the model on train.



## model(ipynb) - evaluation metric selected & supported

You clearly communicate how you evaluated and compared models. What metric(s) did you use and why? For example, in one case, you may decide to use precision over accuracy. If so, why? If you use multiple metrics, how do you decide which to select if metric is better for model A but another metric is better for model B? Do you rank them? Do you find a way to aggregate them into a single metric?
/2


- Eval metric(s) is clear and proper

    > 2 pts  
    > You identified what metric(s) you used to evaluate your models and why. If you considered multiple metrics, you explained how you selected a metric when one was better for model A and another is better for model B.


- Metric(s) used is not appropriate

    > 1 pt
    > You identified how you evaluated your models, but the method selected was not appropriate for the problem at hand.


- Eval methods not documented

    > 0 pts  
    > You did NOT explicitly identify what metric(s) you used to evaluate your models and why. If you considered multiple metrics, you did not explain how you selected a metric when one was better for model A and another is better for model B.



## model(ipynb) - out-of-sample evaluation for expected future performance (test)

Your top performing model, and only your top performing model, should be evaluated on your test dataset. Evaluating only your final model on a test dataset allows you to estimate how the model will perform in the future on unseen data.
/2


- final model evaluated on test

    > 2 pts  
    > Top model evaluated on test: we evaluate one last time on test in order to provide an estimate as to how we expect the model to perform in the future, in production, on data it has never seen.


- multiple models or no model evaluated on test

    > 0 pts  
    > Your top performing model, and only your top performing model should be evaluated on your test dataset. The purpose of having a test dataset to evaluate only the final model on is to have an estimate of how the model will perform in the future on data it has never seen.



## model(ipynb) - unique models developed

Models can differ based on the features used, the hyperparameters selected, and/or the algorithm used to fit the data.
/3


- Developed 4+ Unique Models

    > 3 pts  
    > You developed at least 4 unique models. Models can be different based on the features used, the hyperparameters selected, and/or the algorithm used to fit the data,


- Less than 4 Unique Models

    > 0 pts  
    > You did not develop at least 4 unique models.



# Report
## report(ipynb) - report contains ample code comments

Your code contains code comments that are helpful to the reader in understanding what each blocks/lines of code are doing.
/6


- Adequate comments

    > 6 pts  
    > Your code contains code comments that are helpful to the reader in understanding what your blocks/lines of code are doing.


- Need more code comments

    > 0 pts  
    > Your code needs more commenting to help the reader understand what blocks/lines of code are doing.



## report(ipynb) - report contains ample markdown

Notebook contains adequate markdown that documents your thought process, decision making, and navigation through the pipeline. This should be present throughout the notebook consistently, wtih not just headers, but plenty of content that guides the reader and leaves no questions or doubt as to why you did something, e.g.
/6


- Adequate md documentation

    > 6 pts  
    > Notebook contains adequate markdown that documents your thought process, navigating through the pipeline. This helps your reader not get lost in the details.


- Need more md documentation

    > 0 pts  
    > Your final report/notebook needs more markdown that documents your thought process and helps navigate the reader/end-user through the report. This helps your reader not get lost in the details.



## report(ipynb) - report summarized in a conclusion

Your conclusion summary should addresses the questions you raised in the opening of the project, which we would want to see at the end of every final notebook. Ideally, when the deliverable is a report, the summary should tie together your analysis, the drivers of the outcome, and how you would expect your ML model to perform in the future on unseen data, in layman's terms.
/2


- Includes summary

    > 2 pts  
    > Your conclusion summary addresses the questions you raised in the opening of the project, which we would want to see at the end of every report notebook. Ideally, when the deliverable is a report such as this, the summary should tie together your analysis, the drivers of the outcome, and how you would expect your ML model to perform in the future on unseen data, in layman's terms.


- Missing Summary

    > 0 pts  
    > Your conclusion is missing a summary that brings everything full circle and addresses the questions you raised in the opening of the project. A summary is not the evaluation outcome on the test dataset, it should tie together your analysis, the drivers of the outcome, and how you would expect your ML model to perform in the future on unseen data, in layman's terms.



## report(ipynb) - conclusion contains acctionable recommendations

Your notebook should ends with a conclusion that contains actionable recommendations based on your insights and analysis to the business stakeholder(s), your simulated audience, or someone who would find this information valuable (if there is no stakeholder). Your recommendations should not be not about what to do differently with the data, but instead should be based on the business or domain you are studying.
/2


- Includes recommendations

    > 2 pts  
    > In addition to your summary, you included actionable recommendations based on your insights and analysis to your simulated audience, or someone who would find this information valuable. Your recommendations were not about what to do differently with the data, but instead are based on the business or domain you are studying. Nice work.


- Missing actionable recommendations

    > 0 pts  
    > In addition to the summary, the conclusion should always include actionable recommendations based on your insights and analysis to your simulated audience, or someone who would find this information valuable. Recommendations are not about what to do differently with the data, but instead are based on the business or domain you are studying.


## report(ipynb) - conclusion contains next steps

Your conclusion should include next steps from a data science perspective that will assist in improving your research. Ideally, if you talk about trying more algorithms to improve performance, think about why you need to improve performance. And if the business calls for it, remember the best way to improve performance is to have better predictors/features. If you talk about gathering more data, being specific about what data you think will help you understand the problem better and why is the way to go!
/2


- Includes next steps

    > 2 pts  
    > Your conclusion includes next steps from a data science perspective that will assist in improving your research. Ideally, if you talk about trying more algorithms to improve performance, think about why you need to improve performance. And if the business calls for it, remember the best way to improve performance is to have better predictors/features. If you talk about gathering more data, being specific about what data you think will help you understand the problem better and why is the way to go!


- Missing next steps

    > 0 pts  
    > Your conclusion should also include next steps are steps from a data science perspective that will assist in finding more insights. If you talk about trying more algorithms to improve performance, think about why you need to improve performance. And if the business calls for it, remember the best way to improve performance is to have better predictors/features, not throwing more algorithms at it. If you talk about gathering more data, be more specific, what data do you think will help you understand the problem better and why.

## report(ipynb) - report runs without errors

Your final notebook should run without error. One error in a notebook can lead to the rest of it erroring out. If you have a reader who doesn't know python, they will then not be able to consume your report.
/5


- No Errors

    > 5 pts  
    > Your final notebook runs without error.


- Errors halting progress

    > 0 pts  
    > Your final report couldn't be reproduced because it was halted due to errors.






